{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 웹 브라우저로 웹 사이트 접속하기\n",
    "import webbrowser\n",
    "\n",
    "url = \"www.naver.com\"\n",
    "webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser as web\n",
    "\n",
    "naver_search_url = \"http://search.naver.com/search.naver?query=\"\n",
    "search_word = \"파이썬\"\n",
    "\n",
    "url = naver_search_url + search_word\n",
    "web.open_new(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_url = \"www.google.com/#q=\"\n",
    "search_word = \"python\"\n",
    "url = google_url + search_word\n",
    "\n",
    "web.open_new(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 여러개의 웹 사이트 접속\n",
    "urls = [\"naver.com\", \"daum.net\", \"google.com\"]\n",
    "\n",
    "for url in urls:\n",
    "    web.open_new(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_word = ['python web scraping', 'python webbrowser']\n",
    "\n",
    "for word in search_word:\n",
    "    web.open_new(google_url + word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 웹 스크레핑을 위한 기본 지식\n",
    "# HTTP : Hyper text Transfer Protocol의 약자로 인터넷 상에서 HTML 문서의 정보를 주고 받을 수 있도록 만든 프로토콜\n",
    "# HTML : Hyper text Markup Language의 약자로 웹 페이지의 구조적 구성을 위한 언어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HTML_example.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile HTML_example.html\n",
    "<!doctype html>\n",
    "<html>\n",
    "<head>\n",
    "<meta charset=\"utf-8\">\n",
    "<title>이것은 HTML 예제</title>\n",
    "</head>\n",
    "<body>\n",
    "<h1>출간된 책 정보</h1>\n",
    "<p id = \"book_title\">이해가 쏙쏙 도는 파이썬</p>\n",
    "<p id = \"author\">홍길동</p>\n",
    "<p id = \"publisher\">위키북스 출판사</p>\n",
    "<p id = \"year\">2018</p>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 홈페이지 HTML 소스코드를 가져오기 위한 requests 라이브러리 사용\n",
    "import requests\n",
    "\n",
    "r = requests.get(\"https://www.google.co.kr\")\n",
    "r\n",
    "# 접속이 잘 됐다면 Resonsep[200]반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"ko\"><head><meta content'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.text[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"ko\"><head><meta content'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = requests.get(\"https://www.google.co.kr\").text\n",
    "html[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HTML 코드 구문을 이해하고 요소별로 코드를 분류하는 작업을 'Parsing'이라고 칭함\n",
    "## Beautiful Soup 라이브러리를 통해 파싱을 더 쉽게 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><div><span> <a href=\"http://www.naver.com\">naver</a> <a href=\"http://www.google.com\">google</a> <a href=\"http://www.daum.net\">daum</a> </span></div></body></html>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#테스트용 html 코드\n",
    "html = \"\"\"<html><body><div><span>\\\n",
    "            <a href=http://www.naver.com>naver</a>\\\n",
    "            <a href=http://www.google.com>google</a>\\\n",
    "            <a href=http://www.daum.net>daum</a>\\\n",
    "        </span></div></body></html>\"\"\"\n",
    "\n",
    "# BeautifulSoup를 이용해 HTML소스 파싱\n",
    "soup = BeautifulSoup(html, 'lxml') # lxml은 HTML소스를 처리하기 위한 parser\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <div>\n",
      "   <span>\n",
      "    <a href=\"http://www.naver.com\">\n",
      "     naver\n",
      "    </a>\n",
      "    <a href=\"http://www.google.com\">\n",
      "     google\n",
      "    </a>\n",
      "    <a href=\"http://www.daum.net\">\n",
      "     daum\n",
      "    </a>\n",
      "   </span>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"http://www.naver.com\">naver</a>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a') # a 태그 요소 반환(가장 첫번째)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naver'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a').get_text() # a 태그 요소 중에서 택스트만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://www.naver.com\">naver</a>,\n",
       " <a href=\"http://www.google.com\">google</a>,\n",
       " <a href=\"http://www.daum.net\">daum</a>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"a\") # a 태그 요소 반환(모두 반환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver\n",
      "google\n",
      "daum\n"
     ]
    }
   ],
   "source": [
    "site_names = soup.find_all(\"a\")\n",
    "for name in site_names:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html2 = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<title>작품과 작가 모음</title>\n",
    "</head>\n",
    " <body>\n",
    " <h1>책 정보</h1>\n",
    " <p id=\"book_title\">토지</p>\n",
    " <p id=\"author\">박경리</p>\n",
    " \n",
    " <p id=\"book_title\">태백산맥</p>\n",
    " <p id=\"author\">조정래</p>\n",
    " \n",
    " <p id=\"book_title\">감옥으로부터의 사색</p>\n",
    " <p id=\"author\">신영복</p>\n",
    " \n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup2 = BeautifulSoup(html2, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>작품과 작가 모음</title>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.title # 타이틀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<h1>책 정보</h1>\n",
       "<p id=\"book_title\">토지</p>\n",
       "<p id=\"author\">박경리</p>\n",
       "<p id=\"book_title\">태백산맥</p>\n",
       "<p id=\"author\">조정래</p>\n",
       "<p id=\"book_title\">감옥으로부터의 사색</p>\n",
       "<p id=\"author\">신영복</p>\n",
       "</body>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>책 정보</h1>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.body.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"author\">박경리</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"author\">조정래</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>,\n",
       " <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all(\"p\", {\"id\":\"book_title\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"author\">박경리</p>, <p id=\"author\">조정래</p>, <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all(\"p\", {\"id\":\"author\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토지/박경리\n",
      "태백산맥/조정래\n",
      "감옥으로부터의 사색/신영복\n"
     ]
    }
   ],
   "source": [
    "book_titles = soup2.find_all(\"p\", {\"id\":\"book_title\"})\n",
    "book_authors = soup2.find_all(\"p\", {\"id\":\"author\"})\n",
    "\n",
    "for book_title, author in zip(book_titles, book_authors):\n",
    "    print(book_title.get_text() + \"/\" + author.get_text()) # 책 제목과 작가 요소 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1>책 정보</h1>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find, find_all 말고도 CSS select(선택자)를 이용하는 방법 있음\n",
    "soup2.select(\"body h1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"author\">박경리</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"author\">조정래</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>,\n",
       " <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.select(\"body p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"author\">박경리</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"author\">조정래</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>,\n",
       " <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.select(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"author\">박경리</p>, <p id=\"author\">조정래</p>, <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.select(\"p#author\") # 태그#id_속성값 으로 해당하는 값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HTML_example_site.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile HTML_example_site.html\n",
    "<!doctype html>\n",
    "<html>\n",
    "    <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>사이트 모음</title>\n",
    "    </head>\n",
    "<body>\n",
    "<p id = \"title\"><b>자주 가는 사이트 모음</b></p>\n",
    "<p id = \"contents\">이곳은 자주 가는 사이트를 모아둔 곳입니다.</p>\n",
    "<a href=\"http://www.naver.com\" class = \"portal\" id =\"naver\">네이버</a><br>\n",
    "<a href=\"https://www.google.com\" class = \"search\" id =\"google\">구글</a><br>\n",
    "<a href=\"http://www.daum.net\" class = \"portal\" id =\"daum\">다음</a><br>\n",
    "<a href=\"http://www.nl.go.kr\" class = \"government\" id =\"nl\">국립중앙도서관</a><br>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"HTML_example_site.html\", encoding=\"utf-8\")\n",
    "\n",
    "html3 = f.read()\n",
    "f.close()\n",
    "\n",
    "soup3 = BeautifulSoup(html3, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>,\n",
       " <a class=\"search\" href=\"https://www.google.com\" id=\"google\">구글</a>,\n",
       " <a class=\"portal\" href=\"http://www.daum.net\" id=\"daum\">다음</a>,\n",
       " <a class=\"government\" href=\"http://www.nl.go.kr\" id=\"nl\">국립중앙도서관</a>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>,\n",
       " <a class=\"portal\" href=\"http://www.daum.net\" id=\"daum\">다음</a>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select(\"a.portal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select(\"a#naver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 웹사이트에서 데이터 가져오기\n",
    "# 웹스크레핑시 주의사항 : \n",
    "# 1. 데이터를 얻기 위한 규칙을 발견할 수 있어야 웹 스크레핑이 편함\n",
    "# 2. 뤱 스크레핑시 해당 웹 사이트에 너무 빈번하게 접근하지 말아야 함\n",
    "# 3. 웹 사이트는 예고 없이 변경 될 수 있으므로 지속적인 관리가 필요\n",
    "# 4. 인터넷 상에 공개된 데이터라고 하더라도 저작권이 있는 경우가 있으므로 미리 확인해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "url = \"https://www.alexa.com/topsites/countries/KR\"\n",
    "\n",
    "html_website = requests.get(url).text\n",
    "soup_website = BS(html_website, \"lxml\")\n",
    "\n",
    "website_ranking = soup_website.select(\"p a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://support.alexa.com/hc/en-us/articles/200444340\" target=\"_blank\">this explanation</a>,\n",
       " <a href=\"/siteinfo/google.com\">Google.com</a>,\n",
       " <a href=\"/siteinfo/naver.com\">Naver.com</a>,\n",
       " <a href=\"/siteinfo/youtube.com\">Youtube.com</a>,\n",
       " <a href=\"/siteinfo/daum.net\">Daum.net</a>,\n",
       " <a href=\"/siteinfo/tmall.com\">Tmall.com</a>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this explanation'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_ranking_address = [elments.get_text() for elments in website_ranking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this explanation',\n",
       " 'Google.com',\n",
       " 'Naver.com',\n",
       " 'Youtube.com',\n",
       " 'Daum.net',\n",
       " 'Tmall.com']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking_address[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Top Sites in South Korea]\n",
      "1 : Google.com\n",
      "2 : Naver.com\n",
      "3 : Youtube.com\n",
      "4 : Daum.net\n",
      "5 : Tmall.com\n",
      "6 : Tistory.com\n",
      "7 : Google.co.kr\n",
      "8 : Sohu.com\n",
      "9 : Facebook.com\n",
      "10 : Qq.com\n"
     ]
    }
   ],
   "source": [
    "# 앞의 코드 통합(Alexa에서 한국의 Site Visit Top List 가져오기)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "url = \"https://www.alexa.com/topsites/countries/KR\"\n",
    "\n",
    "html_website = requests.get(url).text\n",
    "soup_website = BS(html_website, \"lxml\")\n",
    "\n",
    "website_ranking = soup_website.select(\"p a\")\n",
    "website_ranking_address = [elments.get_text() for elments in website_ranking]\n",
    "\n",
    "print(\"[Top Sites in South Korea]\")\n",
    "for k in range(10):\n",
    "    print(\"{0} : {1}\".format(k+1, website_ranking_address[k+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this explanation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naver.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Youtube.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Daum.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tmall.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tistory.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Google.co.kr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sohu.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Facebook.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Website\n",
       "1   this explanation\n",
       "2         Google.com\n",
       "3          Naver.com\n",
       "4        Youtube.com\n",
       "5           Daum.net\n",
       "6          Tmall.com\n",
       "7        Tistory.com\n",
       "8       Google.co.kr\n",
       "9           Sohu.com\n",
       "10      Facebook.com"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "website_dict = {\"Website\" : website_ranking_address}\n",
    "df = pd.DataFrame(website_dict, columns = [\"Website\"], index = range(1, len(website_ranking_address)+1))\n",
    "df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"ellipsis\">눈 (Feat. 이문세)</span>,\n",
       " <span class=\"ellipsis\">기억의 빈자리</span>,\n",
       " <span class=\"ellipsis\">선물</span>,\n",
       " <span class=\"ellipsis\">Beautiful</span>,\n",
       " <span class=\"ellipsis\">좋아</span>,\n",
       " <span class=\"ellipsis\">피카부 (Peek-A-Boo)</span>,\n",
       " <span class=\"ellipsis\">좋니</span>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 주간 음악 순위\n",
    "\n",
    "url = \"https://music.naver.com/listen/history/index.nhn?type=TOTAL&year=2017&month=12&week=1\"\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BS(html_music, \"lxml\")\n",
    "\n",
    "# a의 태그 요소 중에서 class 속성값이 \"_title\"인 것을 찾고 \n",
    "# 그 안에서 span 태그의 요소중에 class 속성값이 \"ellipsis\"인 요소 추출\n",
    "titles = soup_music.select(\"a._title span.ellipsis\")\n",
    "titles[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['눈 (Feat. 이문세)', '기억의 빈자리', '선물', 'Beautiful', '좋아', '피카부 (Peek-A-Boo)', '좋니']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_titles = [title.get_text() for title in titles]\n",
    "music_titles[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"ellipsis\">\n",
       " \t\t\t\n",
       " \t\t\t\n",
       " \t\t\tZion.T\n",
       " \t\t</span>, <span class=\"ellipsis\">\n",
       " \t\t\t\n",
       " \t\t\t\n",
       " \t\t\t나얼\n",
       " \t\t</span>, <span class=\"ellipsis\">\n",
       " \t\t\t\n",
       " \t\t\t\n",
       " \t\t\t멜로망스(Melomance)\n",
       " \t\t</span>, <span class=\"ellipsis\">\n",
       " \t\t\t\n",
       " \t\t\t\n",
       " \t\t\tWanna One(워너원)\n",
       " \t\t</span>, <span class=\"ellipsis\">\n",
       " \t\t\t\n",
       " \t\t\t\n",
       " \t\t\tRed Velvet (레드벨벳)\n",
       " \t\t</span>, <span class=\"ellipsis\">\n",
       " \t\t\t\n",
       " \t\t\t\n",
       " \t\t\t윤종신\n",
       " \t\t</span>, <span class=\"ellipsis\">\n",
       " \t\t\t\n",
       " \t\t\t\n",
       " \t\t\t뉴이스트 W\n",
       " \t\t</span>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists = soup_music.select(\"a._artist span.ellipsis\")\n",
    "artists[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zion.T',\n",
       " '나얼',\n",
       " '멜로망스(Melomance)',\n",
       " 'Wanna One(워너원)',\n",
       " 'Red Velvet (레드벨벳)',\n",
       " '윤종신',\n",
       " '뉴이스트 W',\n",
       " 'TWICE(트와이스)',\n",
       " '어반 자카파(Urban Zakapa)',\n",
       " '뉴이스트 W']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_artists = [artist.get_text().strip() for artist in artists]\n",
    "music_artists[0:10] # 민서가 존재하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"_artist NPI=a:artist,r:1,i:115967\" href=\"/artist/home.nhn?artistId=115967\" title=\"Zion.T\">\n",
       "<span class=\"ellipsis\">\n",
       "\t\t\t\n",
       "\t\t\t\n",
       "\t\t\tZion.T\n",
       "\t\t</span>\n",
       "</a>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists = soup_music.select(\"td._artist a\") # 한 단계 상위 tree로 올라가서 td 값이 _artist에서 a인 값들 \n",
    "artists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a alt=\"\" class=\"NPI=a:layerbtn,r:5\" href=\"javascript:void(0);\" title=\"\">민서</a>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists[4] # 민서가 존재하지 않았던 이유는 span 값이 없기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zion.T',\n",
       " '나얼',\n",
       " '멜로망스(Melomance)',\n",
       " 'Wanna One(워너원)',\n",
       " '민서',\n",
       " 'Red Velvet (레드벨벳)',\n",
       " '윤종신',\n",
       " '뉴이스트 W',\n",
       " 'TWICE(트와이스)',\n",
       " '어반 자카파(Urban Zakapa)']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_artists = [artist.get_text().strip() for artist in artists]\n",
    "music_artists[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 눈 (Feat. 이문세) / Zion.T\n",
      "2: 기억의 빈자리 / 나얼\n",
      "3: 선물 / 멜로망스(Melomance)\n",
      "4: Beautiful / Wanna One(워너원)\n",
      "5: 좋아 / 민서\n",
      "6: 피카부 (Peek-A-Boo) / Red Velvet (레드벨벳)\n",
      "7: 좋니 / 윤종신\n"
     ]
    }
   ],
   "source": [
    "# 종합 코드\n",
    "url = \"https://music.naver.com/listen/history/index.nhn?type=TOTAL&year=2017&month=12&week=1&page=1\"\n",
    "# url = \"https://music.naver.com/listen/history/index.nhn?type=TOTAL&year=2017&month=12&week=1&page=2\"\n",
    "\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BS(html_music, \"lxml\")\n",
    "\n",
    "# a의 태그 요소 중에서 class 속성값이 \"_title\"인 것을 찾고 \n",
    "# 그 안에서 span 태그의 요소중에 class 속성값이 \"ellipsis\"인 요소 추출\n",
    "titles = soup_music.select(\"a._title span.ellipsis\")\n",
    "titles[0:7]\n",
    "\n",
    "music_titles = [title.get_text() for title in titles]\n",
    "music_artists = [artist.get_text().strip() for artist in artists]\n",
    "\n",
    "for k in range(7):\n",
    "    print(\"{0}: {1} / {2}\".format(k+1, music_title[k], music_artists[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_titles_artists={}\n",
    "order = 0\n",
    "\n",
    "for (music_title, music_artist) in zip(music_titles, music_artists):\n",
    "    order += 1\n",
    "    music_titles_artists[order] = [music_title, music_artist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['눈 (Feat. 이문세)', 'Zion.T']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_titles_artists[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 웹 페이지에서 이미지 가져오기\n",
    "import requests\n",
    "\n",
    "url = \"https://www.python.org/static/img/python-logo.png\"\n",
    "image = requests.get(url)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python-logo.png'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_name = os.path.basename(url)\n",
    "image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"C:/Users/GroundPark/Documents/Jupyter/image\"\n",
    "\n",
    "if not os.path.exists(folder) :\n",
    "    os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/GroundPark/Documents/Jupyter/image\\\\python-logo.png'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = os.path.join(folder, image_name)\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageFile = open(image_path, \"wb\") # 바이너리 파일모드로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터를 100000바이트씩 나눠서 내려받고 파일에 순차적으로 저장\n",
    "chunk_size = 1000000\n",
    "for chunk in image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)\n",
    "    \n",
    "imageFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img alt=\"여자, 소녀, 어릿광대, 고양이 눈, 눈, 공상, 까마귀, 나무\" src=\"https://cdn.pixabay.com/photo/2019/08/13/19/51/woman-4404102__340.jpg\" srcset=\"https://cdn.pixabay.com/photo/2019/08/13/19/51/woman-4404102__340.jpg 1x, https://cdn.pixabay.com/photo/2019/08/13/19/51/woman-4404102__480.jpg 2x\"/>,\n",
       " <img alt=\"양, Sheepshead, Schäfchen, 양고기, 동물, 양모\" src=\"https://cdn.pixabay.com/photo/2019/08/10/12/46/sheep-4396840__340.jpg\" srcset=\"https://cdn.pixabay.com/photo/2019/08/10/12/46/sheep-4396840__340.jpg 1x, https://cdn.pixabay.com/photo/2019/08/10/12/46/sheep-4396840__480.jpg 2x\"/>,\n",
       " <img alt=\"젖꼭지, 송버드, 새, 작은 새, 빌, 귀여운, 정원 새, 분기, 자연\" src=\"https://cdn.pixabay.com/photo/2019/08/11/06/48/tit-4398247__340.jpg\" srcset=\"https://cdn.pixabay.com/photo/2019/08/11/06/48/tit-4398247__340.jpg 1x, https://cdn.pixabay.com/photo/2019/08/11/06/48/tit-4398247__480.jpg 2x\"/>]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 여러개의 이미지 가져오기\n",
    "\n",
    "url = \"https://pixabay.com/ko/images/search/?cat=animals\"\n",
    "\n",
    "html_pixabay_image = requests.get(url).text\n",
    "soup_pixabay_image = BS(html_pixabay_image, \"lxml\")\n",
    "pixabay_image_elements = soup_pixabay_image.select(\"img\")\n",
    "pixabay_image_elements[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cdn.pixabay.com/photo/2019/08/13/19/51/woman-4404102__340.jpg'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixabay_image_url = pixabay_image_elements[0].get(\"src\")\n",
    "pixabay_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_image = requests.get(pixabay_image_url)\n",
    "\n",
    "folder = \"C:/Users/GroundPark/Documents/Jupyter/image\"\n",
    "\n",
    "imageFile = open(os.path.join(folder, os.path.basename(pixabay_image_url)), \"wb\")\n",
    "\n",
    "# 이미지 데이터를 100000바이트씩 나눠서 내려받고 파일에 순차적으로 저장\n",
    "chunk_size = 1000000\n",
    "for chunk in image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)\n",
    "    \n",
    "imageFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 파일명: woman-4404102__340.jpg. 내려받기 완료!\n",
      "이미지 파일명: sheep-4396840__340.jpg. 내려받기 완료!\n",
      "이미지 파일명: tit-4398247__340.jpg. 내려받기 완료!\n",
      "이미지 파일명: panda-4404234__340.jpg. 내려받기 완료!\n",
      "이미지 파일명: cat-4404664__340.jpg. 내려받기 완료!\n",
      "이미지 파일명: emotions-4403201__340.jpg. 내려받기 완료!\n",
      "이미지 파일명: zebra-4404107__340.jpg. 내려받기 완료!\n"
     ]
    }
   ],
   "source": [
    "## 반복문으로 지정한 개수만큼 이미지를 내려받는 코드 작성\n",
    "\n",
    "# url에서 이미지 주소 추출\n",
    "def get_image_url(url):\n",
    "    html_pixabay_image = requests.get(url).text\n",
    "    soup_pixabay_image = BS(html_pixabay_image, \"lxml\")\n",
    "    pixabay_image_elements = soup_pixabay_image.select(\"img\")\n",
    "    if pixabay_image_elements != None:\n",
    "        image_urls = []\n",
    "        for image_element in pixabay_image_elements:\n",
    "            image_urls.append(image_element.get(\"src\"))\n",
    "        return image_urls\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# 폴더를 지정해 이미지 주소에서 이미지 내려받기\n",
    "def download_image(img_folder, img_url):\n",
    "    if img_url != None:\n",
    "        image = requests.get(url)\n",
    "        \n",
    "        # os.path.basename(URL)은 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리\n",
    "        imageFile = open(os.path.join(img_folder, os.path.basename(img_url)), \"wb\")\n",
    "\n",
    "        # 이미지 데이터를 100000바이트씩 나눠서 내려받고 파일에 순차적으로 저장\n",
    "        chunk_size = 1000000\n",
    "        for chunk in image.iter_content(chunk_size):\n",
    "            imageFile.write(chunk)\n",
    "            imageFile.close()\n",
    "        print(\"이미지 파일명: {0}. 내려받기 완료!\".format(os.path.basename(img_url)) )\n",
    "    else:\n",
    "        print(\"내려 받을 이미지가 없습니다.\" )\n",
    "        \n",
    "pixabay_url = \"https://pixabay.com/ko/images/search/?cat=animals\"\n",
    "\n",
    "figure_folder = \"C:/Users/GroundPark/Documents/Jupyter/image\"\n",
    "pixabay_image_urls = get_image_url(pixabay_url)\n",
    "\n",
    "num = 7\n",
    "\n",
    "for k in range(num):\n",
    "    download_image(figure_folder, pixabay_image_urls[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
